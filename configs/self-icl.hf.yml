exp_name: "self-icl-llama"
# data paths
task_input_path: "./bbh/BIG-Bench-Hard/bbh"
task_desc_path: "./bbh/bbh_task_description_HF.json"
few_shots_path: ""
log_path: "./log/hf"
# experiment settings
inference_mode: "stream"
exemplars_mode: "up-icl"
num_demos: 3
use_cot: False
diverse_exemplars: True
label_method: "self"
# sizes
batch_size: 1
test_sample_size: "full"
# model hparams
model: "meta-llama/Meta-Llama-3-8B-Instruct"
max_tokens: 1024
temperature: 1.0 # temperature when performing inference
demo_temperature: 0.0 # temperature when generating pseudo-demos (only used when exemplars_mode == "self-icl")
top_p: 1.0
